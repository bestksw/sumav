'''
It preprocesses VirusTotal FileFeed data to RDB.
'''
# Default packages
import os
import json
import logging
import tarfile
import threading
import multiprocessing as mp
from datetime import timezone, datetime

# 3rd-party packages
from psycopg2.extras import RealDictCursor

# Internal packages
from sumav.preprocessing.base import PreprocessingBase
from sumav.utils import explore_dir

logger = logging.getLogger(__name__)


class FromVirusTotalFileFeed(PreprocessingBase):
    _bytea_cols = ['md5', 'sha1', 'sha256']

    def convert(self, target_root_path, processes=min(os.cpu_count(), 10)):
        '''Convert from VirusTotal file feed data file to detection table.

        :param str target_root_path: Directory paths of VirusTotal file feed
            data
        '''
        inque, outque = mp.Queue(10), mp.Queue(10)

        # Put file paths in inque.
        th = threading.Thread(target=self.__put_in_que,
                              args=(target_root_path, inque, processes))
        th.start()

        # Start unzip worker.
        for _ in range(processes):
            pr = mp.Process(target=self.__unzip_worker, args=(inque, outque))
            pr.start()

        # Insert rows generated by unzip worker in RDB
        terminated = 0
        num_executed = 0
        while True:
            dt_pkg, rows = outque.get()
            if dt_pkg is None and rows is None:
                terminated += 1
                if terminated == processes:
                    break
                else:
                    continue

            sql, vals = self._make_insert_sql(rows, dt_pkg)
            with self._conn.cursor() as cur:
                cur.execute(sql, vals)
            num_executed += 1
            if num_executed >= 10:
                self._conn.commit()
                num_executed = 0

        if num_executed > 0:
            self._conn.commit()

    def __put_in_que(self, target_root_path, inque, processes):
        # Traverse files in directories
        logger.info('Start explore directories.')
        fps = sorted(explore_dir(target_root_path))

        # Get last package name
        with self._conn.cursor(cursor_factory=RealDictCursor) as cur:
            cur.execute('SELECT * FROM file_feed_log ORDER BY package DESC'
                        ' LIMIT 1')
            if cur.rowcount > 0:
                dt_last_pkg = cur.fetchone()['package']
            else:
                dt_last_pkg = None

        logger.info('dt_last_pkg is %s' % dt_last_pkg)

        # Put file paths in inque
        for i, fp in enumerate(fps, 1):
            # Check already processed
            fn = os.path.basename(fp)
            if not (fn.startswith('file-') and fn.endswith('.tar.bz2')):
                logger.warn(' %s file is not VT feed file..' % fp)
                continue

            if len(fn) == 26:
                dt_pkg = datetime.strptime(fn[5:-8], '%Y%m%dT%H%M').replace(
                                                        tzinfo=timezone.utc)
                if dt_last_pkg is not None and dt_pkg <= dt_last_pkg:
                    continue  # Skip without notification
                if i == 19:
                    print()
                if self.__is_done(dt_pkg):
                    logger.info(' %s already processed.. skipping..' % fn)
                    continue

            # Put the file path to process
            inque.put(fp)
            logger.info('%8s/%s %s processing..' % (i, len(fps), fp))

        # Put end delimiter
        for _ in range(processes):
            inque.put(None)
        if len(fps) > 0:
            logger.info('%8s/%s detection processed.' % (i, len(fps)))

    def __unzip_worker(self, inque, outque):
        while True:
            fp = inque.get()
            if fp is None:
                break

            rows = []
            with tarfile.open(fp, mode='r:bz2') as tar:  # Unzip data
                for member in tar.getmembers():
                    dt_pkg = datetime.strptime(member.name[5:], '%Y%m%dT%H%M')

                    for line in tar.extractfile(member):
                        item_json = line.decode().strip()
                        if not item_json:
                            continue

                        row = json.loads(item_json)
                        if not row['submission']:  # Data is not valid
                            continue
                        if row['positives'] == 0:  # Detected files only
                            continue

                        rows.append(row)

                    if len(rows) > 0:
                        outque.put((dt_pkg, rows))

        outque.put((None, None))  # Terminated signal

    def __is_done(self, package):
        with self._conn.cursor() as cur:
            cur.execute('SELECT 1 FROM file_feed_log WHERE package=%s',
                        [package])
            if cur.rowcount:
                return True
            else:
                return False
